---
output: 
  pdf_document:
    latex_engine: pdflatex
title: "S&DS 563 / F&ES 758b - Multivariate Statistics Homework #2\nPrinciple Components Analysis"
author: Lanxin Jiang, Yazhi Sun, Chenglin Lu
date: "`r Sys.Date()`"
---

```{R, echo=F, eval=F}
setwd("~/Documents/S&DS 563/HW2")
library(knitr)
# https://archive.ics.uci.edu/ml/datasets/chronic_kidney_disease
```

```{R, echo=F}
library(pander)
library(dplyr)
library(PerformanceAnalytics)
library(corrplot)
# Multiple plot function
#
# ggplot objects can be passed in ..., or to plotlist (as a list of ggplot objects)
# - cols:   Number of columns in layout
# - layout: A matrix specifying the layout. If present, 'cols' is ignored.
#
# If the layout is something like matrix(c(1,2,3,3), nrow=2, byrow=TRUE),
# then plot 1 will go in the upper left, 2 will go in the upper right, and
# 3 will go all the way across the bottom.
#
multiplot <- function(..., plotlist=NULL, file, cols=1, layout=NULL) {
  library(grid)

  # Make a list from the ... arguments and plotlist
  plots <- c(list(...), plotlist)

  numPlots = length(plots)

  # If layout is NULL, then use 'cols' to determine layout
  if (is.null(layout)) {
    # Make the panel
    # ncol: Number of columns of plots
    # nrow: Number of rows needed, calculated from # of cols
    layout <- matrix(seq(1, cols * ceiling(numPlots/cols)),
                    ncol = cols, nrow = ceiling(numPlots/cols))
  }
  
  

 if (numPlots==1) {
    print(plots[[1]])

  } else {
    # Set up the page
    grid.newpage()
    pushViewport(viewport(layout = grid.layout(nrow(layout), ncol(layout))))

    # Make each plot, in the correct location
    for (i in 1:numPlots) {
      # Get the i,j matrix positions of the regions that contain this subplot
      matchidx <- as.data.frame(which(layout == i, arr.ind = TRUE))

      print(plots[[i]], vp = viewport(layout.pos.row = matchidx$row,
                                      layout.pos.col = matchidx$col))
    }
  }
}

gg.CSQPlot<-function(vars, label="Chi-Square Quantile Plot"){
  #usually, vars is xxx$residuals or data from one group and label is for plot
  x<-cov(scale(vars),use="pairwise.complete.obs")
  squares<-sort(diag(as.matrix(scale(vars))%*%solve(x)%*%as.matrix(t(scale(vars)))))
  quantiles<-quantile(squares)
  hspr<-quantiles[4]-quantiles[2]
  cumprob<-c(1:length(vars[,1]))/length(vars[,1])-1/(2*length(vars[,1]))
  degf<-dim(x)[1]
  quants<-qchisq(cumprob,df=degf)
  gval<-(quants**(-1+degf/2))/(exp(quants/2)*gamma(degf/2)*(sqrt(2)**degf))
  scale<-hspr / (qchisq(.75,degf)-qchisq(.25,degf))
  se<-(scale/gval)*sqrt(cumprob*(1-cumprob)/length(squares))
  lower<-quants-2*se
  upper<-quants+2*se
  
  # plot(quants,squares,col='red',pch=19,cex=1.2,xlab="Chi-Square Quantiles",
  #      ylab="Squared MH Distance",main=paste("Chi-Square Quantiles for",label),ylim=range(upper,lower,squares),xlim=range(c(0,quants)))
  d <- data.frame(quants, squares, lower, upper)
  rownames(d) <- NULL
  ggplot(d, aes(x=quants, y=squares)) + geom_point() + xlim(range(c(0,quants))) + ylim(range(upper,lower,squares)) + ylab("Squared MH Distance") + xlab("Chi-Square Quantiles") +
    geom_line(aes(x=quants, y=quants)) + 
    geom_ribbon(aes(ymin=lower, ymax=upper), alpha=0.3, show.legend=TRUE) +
    ggtitle(label)
  # lines(c(0,100),c(0,100),col=1)
  # lines(quants,upper,col="blue",lty=2,lwd=2)
  # lines(quants,lower,col="blue",lty=2,lwd=2)
  # legend("topleft",c("Data","95% Conf Limits"),lty=c(0,2),col=c("red","blue"),lwd=c(2,2),
  #        pch=c(19,NA))
}
```

1.  First, discuss whether your data seems to have a multivariate normal distribution. Make univariate plots (boxplots, normal quantile plots as appropriate). Then make transformations as appropriate. You do NOT need to turn all this in, but describe what you did. 

    ```{R, fig.width=5, fig.height=3.5, fig.align="center"}
    library(RWeka)
    library(ggplot2)

    CKD <- read.arff("../Chronic_Kidney_Disease/chronic_kidney_disease_full.arff")

    # Only Numeric Variables
    CKD.numeric <- CKD[,c(1:2,10:18)]
    # Remove Missing Observations
    CKD.numeric <- CKD.numeric[complete.cases(CKD.numeric),]
    #check QQ plot for some numeric variables
    gg.qqplot <- function (variable, title) {
        ggplot(CKD.numeric, aes_string(sample=variable)) + ggtitle(title) + stat_qq()
    }
    p1 <- gg.qqplot("age", "Age")
    p2 <- gg.qqplot("bgr", "Blood Glucose Random")
    p3 <- gg.qqplot("hemo", "Hemoglobin")
    p4 <- gg.qqplot("sc", "Serum Creatinine")
    multiplot(p1, p2, p3, p4, cols=2)
    ```

look for non-linearity, get correlation, make histograms all at once
```{r}
chart.Correlation(CKD.numeric, histogram=TRUE, pch=19)
```
From the above QQ plot and histograms, we could observe that most of our variables don't follow the univariate normal distribution. Age and RBCC seem to be normal. Some of the other variables are left-skewed, while some are right-skewed.  

    **THEN** make a chi-square quantile plot of the data. Turn in your chi-square quantile plot as appropriate and comment on what you see.

    **NOTE that multivariate normality is NOT a requirement for PCA to work!**

    ```{R, fig.width=4, fig.height=4, fig.align="center"}
    gg.CSQPlot(CKD.numeric,label="Chronic Kidney Disease")
    gg.CSQPlot(log(CKD.numeric),label="Chronic Kidney Disease")

    ```
Our data doesn't have multivariat normality. In addition, log transformation doesn't apply to this data. By testing other transformation methods, we conclude that it isn't appropriate to use transformation to achieve normality for our data.

2. Compute the correlation matrix between all variables (SAS and SPSS will provide this for you as part of the PCA procedure in SPSS, click on DESCRIPTIVES. In R use the cor() function.). Comment on relationships you do/do not observe. Do you think PCA will work well?

    ```{R}
    library(ggcorrplot)
    CKD.numeric %>%
      cor(use="pairwise.complete.obs", method = "pearson") %>%
      ggcorrplot(hc.order = TRUE, type = "lower", # lab = TRUE,
        outline.col = "white",
        ggtheme = ggplot2::theme_gray,
        colors = c("#6D9EC1", "white", "#E46726"))
    ```
From the above correlation matrix plot, the more "orange" indicates that two variables more positively correlated, and the more "blue" indicates more negative correlation. We could observe some negative correlation and some positive ones, but nearly half of them have very little correlation. PCA could work for this data because of some of the correlation, but it might not work so effectively.  


3. Perform Principle components analysis using the Correlation matrix (standardized variables). Think about how many principle components to retain.  To make this decision look at
- Total variance explained by a given number of principle components
- The 'eigenvalue > 1' criteria
- The 'scree plot elbow' method  (turn in the scree plot)
- Parallel Analysis: think about whether this is appropriate based on what you discover in number 1.

    ```{R}
    pc1=princomp(CKD.numeric, cor=TRUE)

    #print results
    print(summary(pc1), digits=2, loadings=pc1$loadings, cutoff=0)
    #view eigenvalues as the square of the square-root of eigenvalues 
    pc1$sdev^2
    #Comp1 and Comp2 larger than 1

    #make a screeplot  
    screeplot(pc1,type="lines",col="red",lwd=2,pch=19,cex=1.2,main="Scree Plot")
    #perform parallel analysis
  #get the function online
  source("http://www.reuningscherer.net/STAT660/R/parallel.r.txt")
  #make the parallel analysis plot
  parallelplot(pc1)
  #this plot is not appropriate here because nonnormality.
    ```
The first two compenents explain 54% variability. 91% of the overall variability is explained by the first seven principal components. 
The first two eigenvalues are larger than 1. 
The 'scree plot elbow' method shows that the first two components should retain.
Even thought the parallel analysis also shows the first two components should retain, this method isn't appropriate due to nonnormality.





4. For principle components you decide to retain, examine the loadings (principle components) and think about an interpretation for each retained component if possible.

```{r}
#view loadings
    pc1$loadings
```
Component one is mostly hemoglobin(hemo) and packed cell volume(pcv), component two is mostly age, potassium(pot), white blood cell count(wbcc). Here our cutoff is 0.4. These two components can explain 54% of the variability. There might be clinical explanation for this result. Kidney experts could dig deeper into this phenomenon.


5. Make a score plot of the scores for at least two pairs of component scores (one and two, one and three, two and three, etc).  Discuss any trends/groupings you observe. **As a bonus, try to make a 95% Confidence Ellipse for two of your components**. You might want to also try making a bi-plot if you are using R.



```{r}
#make scoreplot with confidence ellipse : 
#  c(1,2) specifies to use components 1 and 2
#get function from online
source("http://reuningscherer.net/stat660/r/ciscoreplot.R.txt")
#run the function
ciscoreplot(pc1,c(1,2),c(1:pc1$n.obs))
ciscoreplot(pc1,c(1,3),c(1:pc1$n.obs))
#make a biplot for first two components
biplot(pc1,choices=c(1,2),pc.biplot=T)

```
By projecting our data onto the two-dimensional subspace defined by the first and second, or the first and the third principal component axes, we could observe obvious groupings of the black points and  some outliers(blue plots), with the red circle as the 95% confidence ellipse. However, the score plot isn't appropraite to our data because our variables don't follow the multivariate normal distribution. 
The biplot shows the plot of the principal component scores onto which is superimposed a vector representing the coefficients for each of the original variables. Hemoglobin(hemo) and packed cell volume(pcv) are the main vectors in the horizontal (PC1) direction, so we know that the points 254, 247 have unusually small values for these variables.


6. Write a paragraph summarizing your findings, and your opinions about the effectiveness of using principle components on this data.  Include evidence based on scatterplots of linearity in higher dimensional space, note any multivariate outliers in your score plot, comment on sample size relative to number of variables, etc.

The first two components can explain 54% of the variability. Our dataset has obvious correlation between some variables but not among others. Scatterplots of linearity shows variables have linearity but may not be obvious enough. We found some outliers in our score plot and bi-plot. We have 215 observations and 11 variables in the complete case. The ratio of observation to variable is larger than 10, so this sample size works well for PCA. Overall, PCA is effective for this data because of high correlation between some variables, but this method has some limitations in that there is little correlation between certain variables and nonnormality of our variables. 
